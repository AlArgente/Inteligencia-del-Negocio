test_train$MoSold <- as.character(test_train$MoSold)
test_train$YrSold <- as.character(test_train$YrSold)
cols = c("FireplaceQu", "BsmtQual", "BsmtCond", "GarageQual", "GarageCond",
"ExterQual", "ExterCond", "HeatingQC", "PoolQC", "KitchenQual", "BsmtFinType1",
"BsmtFinType2", "Functional", "Fence", "BsmtExposure", "GarageFinish",
"LandSlope", "LotShape", "PavedDrive", "Street", "Alley", "CentralAir",
"MSSubClass", "OverallCond", "YrSold", "MoSold")
FireplaceQu = c("None", "Po", "Fa", "TA", "Gd", "Ex")
BsmtQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
BsmtCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
ExterQual = c("Po", "Fa", "TA", "Gd", "Ex")
ExterCond = c("Po", "Fa", "TA", "Gd", "Ex")
HeatingQC = c("Po", "Fa", "TA", "Gd", "Ex")
PoolQC = c("None", "Fa", "TA", "Gd", "Ex")
KitchenQual = c("Po", "Fa", "TA", "Gd", "Ex")
BsmtFinType1 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
BsmtFinType2 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
Functional = c("Sal", "Sev", "Maj2", "Maj1", "Mod", "Min2", "Min1", "Typ")
Fence = c("None", "MnWw", "GdWo", "MnPrv", "GdPrv")
BsmtExposure = c("None", "No", "Mn", "Av", "Gd")
GarageFinish = c("None", "Unf", "RFn", "Fin")
LandSlope = c("Sev", "Mod", "Gtl")
LotShape = c("IR3", "IR2", "IR1", "Reg")
PavedDrive = c("N", "P", "Y")
Street = c("Pave", "Grvl")
Alley = c("None", "Pave", "Grvl")
MSSubClass = c("20", "30", "40", "45", "50", "60", "70", "75", "80", "85",
"90", "120", "150", "160", "180", "190")
OverallCond = NA
MoSold = NA
YrSold = NA
CentralAir = NA
levels = list(FireplaceQu, BsmtQual, BsmtCond, GarageQual, GarageCond,
ExterQual, ExterCond, HeatingQC, PoolQC, KitchenQual, BsmtFinType1,
BsmtFinType2, Functional, Fence, BsmtExposure, GarageFinish, LandSlope,
LotShape, PavedDrive, Street, Alley, CentralAir, MSSubClass, OverallCond,
YrSold, MoSold)
i = 1
for (c in cols) {
if (c == "CentralAir" | c == "OverallCond" | c == "YrSold" | c == "MoSold") {
test_train[, c] = as.numeric(factor(test_train[, c]))
} else {
test_train[, c] = as.numeric(factor(test_train[, c], levels = levels[[i]]))
#cat("Estoy en else", c, "\n")
}
i = i + 1
}
test_train$TotalSF <- test_train$TotalBsmtSF + test_train$X1stFlrSF + test_train$X2ndFlrSF
# Primero obtenemos el tipo.
clases_caracteristicas <- sapply(names(test_train), function(x) {
class(test_train[[x]])
})
numeric_vars <- names(clases_caracteristicas[clases_caracteristicas != "character"])
categorical_vars <- names(clases_caracteristicas[clases_caracteristicas == "character"])
library(caret)
dummies <- dummyVars(~., test_train[categorical_vars])
categoricas <- predict(dummies, test_train[categorical_vars])
library(MASS)
library(moments)
skewed_vars<- sapply(numeric_vars, function(x) {
skewness(test_train[[x]], na.rm = TRUE)
})
## Mantenemos solo aquellas que superen 0.75 de skewness
skewed_vars <- skewed_vars[abs(skewed_vars) > 0.75]
## Transformamos estas variables con la transformaciÃ³n Box-Cox
for (x in names(skewed_vars)) {
bc = BoxCoxTrans(test_train[[x]], lambda = 0.15)
test_train[[x]] <- predict(bc, test_train[[x]])
}
test_train <- cbind(test_train[numeric_vars], categoricas)
dim(test_train)
training <- test_train[1:1458, ]
testing <- test_train[1459:2917, ]
# ParticiÃ³n para el entrenamiendo de los modelos
pTrain <- createDataPartition(y = training$SalePrice, p = 0.8, list = FALSE)
Training <- training[pTrain, ]
Validation <- training[-pTrain, ]
library(Metrics)
library(gbm)
library(xgboost)
library(glmnet)
library(caret)
set.seed(371)
gbmFit <- gbm(SalePrice ~., data = Training, shrinkage = 0.05, interaction.depth=3,
bag.fraction = 0.5, n.minobsinnode = 10,
cv.folds = 5, verbose=FALSE, distribution = "gaussian", n.trees = 3000)
predsp1 <- predict(gbmFit, newdata = Validation)
rmse(Validation$SalePrice, predsp1)
set.seed(793)
lassoFit <- cv.glmnet(x = as.matrix(Training[,-59]), y = as.matrix(Training[,59]), family = "gaussian",
alpha =0.1655, nfolds = 5)
predsp2 <- predict(lassoFit, newx = as.matrix(Validation[,-59]), s = "lambda.min")
rmse(Validation$SalePrice, predsp2)
set.seed(371)
xgbtrain <- xgb.DMatrix(data = as.matrix(training[,-59]), label = as.matrix(training$SalePrice))
params <- list(booster = "gbtree", objetive = "reg:linear", eta = 0.01, gamma = 0.01,
max_depth = 3, min_child_weight = 1.782, subsample = 0.5213,
colsample_bytree=0.4578, eval_metric='rmse', nthread = 4, verbose=TRUE, seed=371,
alpha = 0.4640, lambda = 0.8574, silent = 1)
xgbcvFited <- xgb.cv( params = params, data = xgbtrain, nrounds = 3200, nfold = 5, stratified = T, print_every_n = 100,
early_stopping_rounds = 10, maximize =F, prediction = TRUE)
set.seed(193)
## Preparando la matriz
dtrain <- xgb.DMatrix(data = as.matrix(Training[,-59]), label = as.matrix(Training$SalePrice))
dtest <- xgb.DMatrix(data = as.matrix(Validation[,-59]), label = as.matrix(Validation$SalePrice))
## ParÃ¡metros para empezar
params <- list(booster = "gbtree", objetive = "reg:linear", eta = 0.01, gamma = 0.01,
max_depth = 3, min_child_weight = 1.782, subsample = 0.5213,
colsample_bytree=0.4578, eval_metric='rmse', nthread = 4, verbose=TRUE, seed=371,
alpha = 0.4640, lambda = 0.8574, silent = 1)
xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 3200, nfold = 5, stratified = T, print_every_n = 100,
early_stopping_rounds = 10, maximize =F, prediction = TRUE)
xgbmatrix <- xgbcv$pred
#predcv <- predict(xgbcv$pred, newdata = as.matrix(Validation[,-59]))
#predcv <- predict(as.matrix(xgbmatrix), newdata = as.matrix(Validation[,-59]))
#rmse(Validation$SalePrice, predcv)
set.seed(371)
xgbFit <- xgboost(data = dtrain, nfold = 5, labels = as.matrix(Training$SalePrice),
nrounds = 2400, verbose = FALSE, objetive = "reg:linear", eval_metric = 'rmse',
nthread = 4, eta = 0.05, gamma = 0.0468, max_depth = 3, min_child_weight = 1.782,
subsample = 0.5213, colsample_bytree = 0.4588, silent = 1, alpha = 0.4640, lambda = 0.8574)
pred <- predict(xgbFit, newdata = as.matrix(Validation[,-59]))
rmse(Validation$SalePrice, pred)
rmse(Validation$SalePrice, (pred*0.3 + predsp1*0.4 + predsp2*0.3))
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
set.seed(137)
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
test <- read.csv(file = "test.csv", stringsAsFactors = FALSE)
str(train)
summary(train)
cat("Algunas estadÃ�sticas de SalePrice: ")
summary(train$SalePrice)
cat("La mediana de Sale Price es: ", median(train$SalePrice))
traid_ID <- train$Id
test_ID <- test$Id
test$SalePrice <- NA
qplot(train$GrLivArea, train$SalePrice, main = "With Outliers")
train <- train[-which(train$GrLivArea > 4000 & train$SalePrice < 3e+05),]
qplot(train$GrLivArea, train$SalePrice, main = "Without Outliers")
qplot(SalePrice, data = train, bins = 50, main = "DistribuciÃ³n sesgada")
train$SalePrice <- log(train$SalePrice + 1)
qplot(SalePrice, data = train, bins = 50, main = "DistribuciÃ³n normal")
#Combinamos train y test
test_train <- rbind(train,test)
#Eliminamos el Id.
test_train <- test_train[,-1]
colSums(is.na(test_train))
# Para algunas variables pondemos NA como None.
for (x in c("Alley", "PoolQC", "MiscFeature", "Fence", "FireplaceQu", "GarageType",
"GarageFinish", "GarageQual", "GarageCond", "BsmtQual", "BsmtCond",
"BsmtExposure", "BsmtFinType1", "BsmtFinType2", "MasVnrType")) {
test_train[is.na(test_train[,x]), x] = "None"
}
# Agrupamos por vecindario.
temp <- aggregate(LotFrontage ~ Neighborhood, data = test_train, median)
temp2 <- c()
for(str in test_train$Neighborhood[is.na(test_train$LotFrontage)]){
temp2 <- c(temp2, which(temp$Neighborhood == str))
}
test_train$LotFrontage[is.na(test_train$LotFrontage)] = temp[temp2, 2]
# Reemplazamos los missing data por 0.
for (col in c("GarageYrBlt", "GarageArea", "GarageCars", "BsmtFinSF1",
"BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath",
"MasVnrArea")) {
test_train[is.na(test_train[, col]), col] = 0
}
# Reemplazamos los NA de MsZoning por RL
test_train$MSZoning[is.na(test_train$MSZoning)] = "RL"
## Eliminamos Utilities porque tiene pocas variedades
test_train <- test_train[,-9]
## Reemplazamos los missing values de Functional por "Typ"
test_train$Functional[is.na(test_train$Functional)] = "Typ"
## Reemplazamos lo smissing value sde Electrical por "SBrkr"
test_train$Electrical[is.na(test_train$Electrical)] = "SBrkr"
## Reemplazamos los missing values de KitchenQual por "TA"
test_train$KitchenQual[is.na(test_train$Electrical)] = "TA"
## Reemplazamos los missing values de SaleType por "WD"
test_train$SaleType[is.na(test_train$SaleType)] = "WD"
## Reemplazamos los missing values de Exterior1st y Exterior2nd por "VinylSd"
test_train$Exterior1st[is.na(test_train$Exterior1st)] = "VinylSd"
test_train$Exterior2nd[is.na(test_train$Exterior2nd)] = "VinylSd"
## Una vez hemos hecho esto no debe haber missing values salvo los NA de SalePrice de la parte de test
colSums(is.na(test_train))
test_train$MSSubClass <- as.character(test_train$MSSubClass)
test_train$OverallCond <- as.character(test_train$OverallCond)
test_train$MoSold <- as.character(test_train$MoSold)
test_train$YrSold <- as.character(test_train$YrSold)
cols = c("FireplaceQu", "BsmtQual", "BsmtCond", "GarageQual", "GarageCond",
"ExterQual", "ExterCond", "HeatingQC", "PoolQC", "KitchenQual", "BsmtFinType1",
"BsmtFinType2", "Functional", "Fence", "BsmtExposure", "GarageFinish",
"LandSlope", "LotShape", "PavedDrive", "Street", "Alley", "CentralAir",
"MSSubClass", "OverallCond", "YrSold", "MoSold")
FireplaceQu = c("None", "Po", "Fa", "TA", "Gd", "Ex")
BsmtQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
BsmtCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageQual = c("None", "Po", "Fa", "TA", "Gd", "Ex")
GarageCond = c("None", "Po", "Fa", "TA", "Gd", "Ex")
ExterQual = c("Po", "Fa", "TA", "Gd", "Ex")
ExterCond = c("Po", "Fa", "TA", "Gd", "Ex")
HeatingQC = c("Po", "Fa", "TA", "Gd", "Ex")
PoolQC = c("None", "Fa", "TA", "Gd", "Ex")
KitchenQual = c("Po", "Fa", "TA", "Gd", "Ex")
BsmtFinType1 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
BsmtFinType2 = c("None", "Unf", "LwQ", "Rec", "BLQ", "ALQ", "GLQ")
Functional = c("Sal", "Sev", "Maj2", "Maj1", "Mod", "Min2", "Min1", "Typ")
Fence = c("None", "MnWw", "GdWo", "MnPrv", "GdPrv")
BsmtExposure = c("None", "No", "Mn", "Av", "Gd")
GarageFinish = c("None", "Unf", "RFn", "Fin")
LandSlope = c("Sev", "Mod", "Gtl")
LotShape = c("IR3", "IR2", "IR1", "Reg")
PavedDrive = c("N", "P", "Y")
Street = c("Pave", "Grvl")
Alley = c("None", "Pave", "Grvl")
MSSubClass = c("20", "30", "40", "45", "50", "60", "70", "75", "80", "85",
"90", "120", "150", "160", "180", "190")
OverallCond = NA
MoSold = NA
YrSold = NA
CentralAir = NA
levels = list(FireplaceQu, BsmtQual, BsmtCond, GarageQual, GarageCond,
ExterQual, ExterCond, HeatingQC, PoolQC, KitchenQual, BsmtFinType1,
BsmtFinType2, Functional, Fence, BsmtExposure, GarageFinish, LandSlope,
LotShape, PavedDrive, Street, Alley, CentralAir, MSSubClass, OverallCond,
YrSold, MoSold)
i = 1
for (c in cols) {
if (c == "CentralAir" | c == "OverallCond" | c == "YrSold" | c == "MoSold") {
test_train[, c] = as.numeric(factor(test_train[, c]))
} else {
test_train[, c] = as.numeric(factor(test_train[, c], levels = levels[[i]]))
#cat("Estoy en else", c, "\n")
}
i = i + 1
}
test_train$TotalSF <- test_train$TotalBsmtSF + test_train$X1stFlrSF + test_train$X2ndFlrSF
# Primero obtenemos el tipo.
clases_caracteristicas <- sapply(names(test_train), function(x) {
class(test_train[[x]])
})
numeric_vars <- names(clases_caracteristicas[clases_caracteristicas != "character"])
categorical_vars <- names(clases_caracteristicas[clases_caracteristicas == "character"])
library(caret)
dummies <- dummyVars(~., test_train[categorical_vars])
categoricas <- predict(dummies, test_train[categorical_vars])
library(MASS)
library(moments)
skewed_vars<- sapply(numeric_vars, function(x) {
skewness(test_train[[x]], na.rm = TRUE)
})
## Mantenemos solo aquellas que superen 0.75 de skewness
skewed_vars <- skewed_vars[abs(skewed_vars) > 0.75]
## Transformamos estas variables con la transformaciÃ³n Box-Cox
for (x in names(skewed_vars)) {
bc = BoxCoxTrans(test_train[[x]], lambda = 0.15)
test_train[[x]] <- predict(bc, test_train[[x]])
}
test_train <- cbind(test_train[numeric_vars], categoricas)
dim(test_train)
training <- test_train[1:1458, ]
testing <- test_train[1459:2917, ]
# ParticiÃ³n para el entrenamiendo de los modelos
pTrain <- createDataPartition(y = training$SalePrice, p = 0.8, list = FALSE)
Training <- training[pTrain, ]
Validation <- training[-pTrain, ]
library(Metrics)
library(gbm)
library(xgboost)
library(glmnet)
library(caret)
set.seed(371)
gbmFit <- gbm(SalePrice ~., data = Training, shrinkage = 0.05, interaction.depth=3,
bag.fraction = 0.5, n.minobsinnode = 10,
cv.folds = 5, verbose=FALSE, distribution = "gaussian", n.trees = 3000)
predsp1 <- predict(gbmFit, newdata = Validation)
rmse(Validation$SalePrice, predsp1)
set.seed(371)
gbmFited <- gbm(SalePrice ~., data = training, shrinkage = 0.01, interaction.depth=3,
bag.fraction = 0.5, train.fraction = 1,  n.minobsinnode = 10,
cv.folds = 5, verbose=FALSE, distribution = "gaussian", n.trees = 3000)
predsGBM <- exp(predict(gbmFited, newdata = testing)) - 1
dfGBM <- data.frame(Id = test_ID, SalePrice = predsGBM)
write.csv(dfGBM, "1stGBM.csv", row.names = FALSE)
set.seed(793)
lassoFit <- cv.glmnet(x = as.matrix(Training[,-59]), y = as.matrix(Training[,59]), family = "gaussian",
alpha =0.1655, nfolds = 5)
predsp2 <- predict(lassoFit, newx = as.matrix(Validation[,-59]), s = "lambda.min")
rmse(Validation$SalePrice, predsp2)
set.seed(793)
lasso.fit <- cv.glmnet(x = as.matrix(training[,-59]), y = as.matrix(training[,59]), nfolds = 5
, family = "gaussian", alpha =0.1655)
preds.lasso <- exp(predict(lasso.fit, newx = as.matrix(testing[,-59]), s = "lambda.min")) - 1
df.lasso <- data.frame(Id = test_ID, SalePrice = as.numeric(preds.lasso))
write.csv(df.lasso, "1stLasso.csv", row.names = FALSE)
set.seed(193)
## Preparando la matriz
dtrain <- xgb.DMatrix(data = as.matrix(Training[,-59]), label = as.matrix(Training$SalePrice))
dtest <- xgb.DMatrix(data = as.matrix(Validation[,-59]), label = as.matrix(Validation$SalePrice))
## ParÃ¡metros para empezar
params <- list(booster = "gbtree", objetive = "reg:linear", eta = 0.01, gamma = 0.01,
max_depth = 3, min_child_weight = 1.782, subsample = 0.5213,
colsample_bytree=0.4578, eval_metric='rmse', nthread = 4, verbose=TRUE, seed=371,
alpha = 0.4640, lambda = 0.8574, silent = 1)
xgbcv <- xgb.cv( params = params, data = dtrain, nrounds = 3200, nfold = 5, stratified = T, print_every_n = 100,
early_stopping_rounds = 10, maximize =F, prediction = TRUE)
xgbmatrix <- xgbcv$pred
#predcv <- predict(xgbcv$pred, newdata = as.matrix(Validation[,-59]))
#predcv <- predict(as.matrix(xgbmatrix), newdata = as.matrix(Validation[,-59]))
#rmse(Validation$SalePrice, predcv)
set.seed(371)
xgbtrain <- xgb.DMatrix(data = as.matrix(training[,-59]), label = as.matrix(training$SalePrice))
params <- list(booster = "gbtree", objetive = "reg:linear", eta = 0.01, gamma = 0.01,
max_depth = 3, min_child_weight = 1.782, subsample = 0.5213,
colsample_bytree=0.4578, eval_metric='rmse', nthread = 4, verbose=TRUE, seed=371,
alpha = 0.4640, lambda = 0.8574, silent = 1)
xgbcvFited <- xgb.cv( params = params, data = xgbtrain, nrounds = 3200, nfold = 5, stratified = T, print_every_n = 100,
early_stopping_rounds = 10, maximize =F, prediction = TRUE)
set.seed(371)
xgbFit <- xgboost(data = dtrain, nfold = 5, labels = as.matrix(Training$SalePrice),
nrounds = 2400, verbose = FALSE, objetive = "reg:linear", eval_metric = 'rmse',
nthread = 4, eta = 0.05, gamma = 0.0468, max_depth = 3, min_child_weight = 1.782,
subsample = 0.5213, colsample_bytree = 0.4588, silent = 1, alpha = 0.4640, lambda = 0.8574)
pred <- predict(xgbFit, newdata = as.matrix(Validation[,-59]))
rmse(Validation$SalePrice, pred)
set.seed(371)
xgbtrain <- xgb.DMatrix(data = as.matrix(training[,-59]), label = as.matrix(training$SalePrice))
xgbFit <- xgboost(data = xgbtrain, nfold = 5, labels = as.matrix(training$SalePrice),
nrounds = 2200, verbose = FALSE, objetive = "reg:linear", eval_metric = 'rmse',
nthread = 4, eta = 0.05, gamma = 0.01, max_depth = 3, min_child_weight = 1.78171,
subsample = 0.5213, colsample_bytree = 0.4603, silent = 1, alpha = 0.4640, lambda = 0.8574)
predXGB <- exp(predict(xgbFit, newdata = as.matrix(testing[, -59]))) - 1
df <- data.frame(Id = test_ID, SalePrice = predXGB)
write.csv(df, "NewXgboost.csv", row.names = FALSE)
rmse(Validation$SalePrice, (pred + predsp1 + predsp2)/3)
rmse(Validation$SalePrice, (pred*0.4 + predsp1*0.3 + predsp2*0.3))
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
set.seed(137)
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
test <- read.csv(file = "test.csv", stringsAsFactors = FALSE)
str(train)
summary(train)
cat("Algunas estadÃ�sticas de SalePrice: ")
summary(train$SalePrice)
cat("La mediana de Sale Price es: ", median(train$SalePrice))
traid_ID <- train$Id
test_ID <- test$Id
test$SalePrice <- NA
help("xgboost")
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
set.seed(137)
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
test <- read.csv(file = "test.csv", stringsAsFactors = FALSE)
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
set.seed(137)
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
test <- read.csv(file = "test.csv", stringsAsFactors = FALSE)
str(train)
summary(train)
cat("Algunas estadÃ�sticas de SalePrice: ")
summary(train$SalePrice)
cat("La mediana de Sale Price es: ", median(train$SalePrice))
traid_ID <- train$Id
test_ID <- test$Id
test$SalePrice <- NA
train <- train[-which(train$GrLivArea > 4000 & train$SalePrice < 3e+05),]
qplot(train$GrLivArea, train$SalePrice, main = "Without Outliers")
qplot(SalePrice, data = train, bins = 50, main = "DistribuciÃ³n sesgada")
train$SalePrice <- log(train$SalePrice + 1)
qplot(SalePrice, data = train, bins = 50, main = "Distribución normal")
help("qplot")
#Combinamos train y test
test_train <- rbind(train,test)
#Eliminamos el Id.
test_train <- test_train[,-1]
colSums(is.na(test_train))
# Para algunas variables pondemos NA como None.
for (x in c("Alley", "PoolQC", "MiscFeature", "Fence", "FireplaceQu", "GarageType",
"GarageFinish", "GarageQual", "GarageCond", "BsmtQual", "BsmtCond",
"BsmtExposure", "BsmtFinType1", "BsmtFinType2", "MasVnrType")) {
test_train[is.na(test_train[,x]), x] = "None"
}
# Agrupamos por vecindario.
temp <- aggregate(LotFrontage ~ Neighborhood, data = test_train, median)
temp2 <- c()
for(str in test_train$Neighborhood[is.na(test_train$LotFrontage)]){
temp2 <- c(temp2, which(temp$Neighborhood == str))
}
test_train$LotFrontage[is.na(test_train$LotFrontage)] = temp[temp2, 2]
# Reemplazamos los missing data por 0.
for (col in c("GarageYrBlt", "GarageArea", "GarageCars", "BsmtFinSF1",
"BsmtFinSF2", "BsmtUnfSF", "TotalBsmtSF", "BsmtFullBath", "BsmtHalfBath",
"MasVnrArea")) {
test_train[is.na(test_train[, col]), col] = 0
}
# Reemplazamos los NA de MsZoning por RL
test_train$MSZoning[is.na(test_train$MSZoning)] = "RL"
## Eliminamos Utilities porque tiene pocas variedades
test_train <- test_train[,-9]
## Reemplazamos los missing values de Functional por "Typ"
test_train$Functional[is.na(test_train$Functional)] = "Typ"
## Reemplazamos lo smissing value sde Electrical por "SBrkr"
test_train$Electrical[is.na(test_train$Electrical)] = "SBrkr"
## Reemplazamos los missing values de KitchenQual por "TA"
test_train$KitchenQual[is.na(test_train$Electrical)] = "TA"
## Reemplazamos los missing values de SaleType por "WD"
test_train$SaleType[is.na(test_train$SaleType)] = "WD"
## Reemplazamos los missing values de Exterior1st y Exterior2nd por "VinylSd"
test_train$Exterior1st[is.na(test_train$Exterior1st)] = "VinylSd"
test_train$Exterior2nd[is.na(test_train$Exterior2nd)] = "VinylSd"
## Una vez hemos hecho esto no debe haber missing values salvo los NA de SalePrice de la parte de test
colSums(is.na(test_train))
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
set.seed(371)
gbmFit <- gbm(SalePrice ~., data = Training, shrinkage = 0.01, interaction.depth=3,
bag.fraction = 0.5, n.minobsinnode = 10,
cv.folds = 5, verbose=FALSE, distribution = "gaussian", n.trees = 3000)
predsp1 <- predict(gbmFit, newdata = Validation)
rmse(Validation$SalePrice, predsp1)
help("gbm")
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
library(mice)
set.seed(137)
# Para algunas variables pondemos NA como None.
for (x in c("Alley", "PoolQC", "MiscFeature", "Fence", "FireplaceQu", "GarageType",
"GarageFinish", "GarageQual", "GarageCond", "BsmtQual", "BsmtCond",
"BsmtExposure", "BsmtFinType1", "BsmtFinType2", "MasVnrType")) {
test_train[is.na(test_train[,x]), x] = "None"
}
colSums(is.na(test_train))
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
library(mice)
set.seed(137)
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
library(mice)
set.seed(137)
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
