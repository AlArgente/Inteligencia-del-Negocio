---
title: "KaggleCompetition"
author: "Alberto Argente"
date: "19 de diciembre de 2017"
output: pdf_document
---

```{r setup, include=FALSE}
library(data.table) # database-like tables
library(ggplot2) # general plots
library(highcharter) # interactive plots
library(DT) # display tables
library(corrplot) # corr SPLOMs
library(vcd) # stats for categories
library(mice) # multiple imputation
library(Boruta) # estimate variable importance
library(dplyr)
library(mice)

set.seed(137)
```


Leemos el train y el test del dataset.

```{r include=FALSE}
train <- read.csv(file = "train.csv", stringsAsFactors = FALSE)
test <- read.csv(file = "test.csv", stringsAsFactors = FALSE)
```

## Preprocesamiento de datos.

Primero guardamos los ID's de las variables de train y de test en unas variables auxiliares y a帽adimos la columna SalePrice a test ya que no tiene ninguna.
```{r}
traid_ID <- train$Id
test_ID <- test$Id
test$SalePrice <- NA
```

### Eliminando outliers.

```{r}
qplot(train$GrLivArea, train$SalePrice, main = "With Outliers")
```

```{r}
train <- train[-which(train$GrLivArea > 4000 & train$SalePrice < 3e+05),]
qplot(train$GrLivArea, train$SalePrice, main = "Without Outliers")
```

Una vez hemos eliminado los outliers procedo a aplicar el logaritmo sobre SalePrice, primero vemos c贸mo quedan los datos previamente:

```{r}
qplot(SalePrice, data = train, bins = 50, main = "Distribuci贸n sesgada")
```


Ahora aplicamos el logaritmo y visualizamos la distribuci贸n normal
```{r}
train$SalePrice <- log(train$SalePrice + 1)
qplot(SalePrice, data = train, bins = 50, main = "Distribuci贸n normal")
```


Una vez hemos hecho esto combinamos test y train y eliminamos la columna de ID que hab铆amos a帽adido.

```{r}
#Combinamos train y test
test_train <- rbind(train,test)
#Eliminamos el Id.
test_train <- test_train[,-1]
```

### Analizando los missing values.

Una vez tenemos el conjunto de datos preparado voy a analizar los missing values de cada una de las variables.

```{r}
colSums(is.na(test_train))
```
Podemos ver que hay variables como PoolQC, Fence o MiscFeature con un 90% da datos como missing values por lo que ahora procedemos a la imputaci贸n de datos.

### Imputaci贸n de datos.

Para la imputaci贸n de datos, como se explica en el documento proporcionado en la plataforma kaggle (data_description), hay muchas variables que tienen valores NA cuando este documento nos est谩 indicando lo contrario que pueden significar 0 o ninguna, por ello la imputaci贸n de datos ser谩 en su mayor铆a independiente de las variables.


```{r}
# Para algunas variables pondemos NA como None.
for (x in c("Alley", "PoolQC", "MiscFeature", "Fence", "FireplaceQu", "GarageType", 
    "GarageFinish", "GarageQual", "GarageCond", "BsmtQual", "BsmtCond", 
    "BsmtExposure", "BsmtFinType1", "BsmtFinType2", "MasVnrType")) {
    test_train[is.na(test_train[,x]), x] = "None"
}
colSums(is.na(test_train))
```

Imputamos con mice:
```{r}
## Eliminamos Utilities porque tiene pocas variedades
test_train <- test_train[,-9]
```

```{r}
t_t <- test_train[,79]
imputados <- mice::mice(test_train[,-79], m=5, method="rf", maxit = 2, seed = 500, diagnostics = TRUE)
datosImputados <- mice::complete(imputados)
colSums(is.na(datosImputados))
```

Como podemos ver an se quedan algunos valores perdidos, que imputaremos por la moda que tienen estas caractersticas y aadiremos la columna SalePrice

```{r}
# Reemplazamos los NA de MsZoning por RL
datosImputados$MSZoning[is.na(datosImputados$MSZoning)] = "RL"

## Reemplazamos los missing values de Functional por "Typ"
datosImputados$Functional[is.na(datosImputados$Functional)] = "Typ"

## Reemplazamos lo smissing value sde Electrical por "SBrkr"
datosImputados$Electrical[is.na(datosImputados$Electrical)] = "SBrkr"

## Reemplazamos los missing values de KitchenQual por "TA"
datosImputados$KitchenQual[is.na(datosImputados$KitchenQual)] = "TA"

## Reemplazamos los missing values de SaleType por "WD"
datosImputados$SaleType[is.na(datosImputados$SaleType)] = "WD"

## Reemplazamos los missing values de Exterior1st y Exterior2nd por "VinylSd"
datosImputados$Exterior1st[is.na(datosImputados$Exterior1st)] = "VinylSd"
datosImputados$Exterior2nd[is.na(datosImputados$Exterior2nd)] = "VinylSd"
test_train <- datosImputados
test_train$SalePrice <- t_t
```

Una vez eliminados los missing values convertimos a categ贸ricas algunas variables que salen como num茅ricas pero realmente son categ贸ricas como por ej MoSold o YrSold.
```{r}
test_train$MSSubClass <- as.character(test_train$MSSubClass)
test_train$OverallCond <- as.character(test_train$OverallCond)
test_train$MoSold <- as.character(test_train$MoSold)
test_train$YrSold <- as.character(test_train$YrSold)
```




A帽adimos una nueva caracter铆stica que nos representa mejor las variables: 

```{r}
test_train$TotalSF <- test_train$TotalBsmtSF + test_train$X1stFlrSF + test_train$X2ndFlrSF
```

Obteniendo las categor铆as dummy.

```{r}
# Primero obtenemos el tipo.
clases_caracteristicas <- sapply(names(test_train), function(x) { 
  class(test_train[[x]])
})

numeric_vars <- names(clases_caracteristicas[clases_caracteristicas != "character"])

categorical_vars <- names(clases_caracteristicas[clases_caracteristicas == "character"])

library(caret)
dummies <- dummyVars(~., test_train[categorical_vars])
categoricas <- predict(dummies, test_train[categorical_vars])
```



```{r}
library(MASS)
library(moments)
skewed_vars<- sapply(numeric_vars, function(x) {
    skewness(test_train[[x]], na.rm = TRUE)
})

## Mantenemos solo aquellas que superen 0.75 de skewness
skewed_vars <- skewed_vars[abs(skewed_vars) > 0.75]

## Transformamos estas variables con la transformaci贸n Box-Cox
for (x in names(skewed_vars)) {
  bc = BoxCoxTrans(test_train[[x]], lambda = 0.15)
  test_train[[x]] <- predict(bc, test_train[[x]])
}
```


```{r}
test_train <- cbind(test_train[numeric_vars], categoricas)
dim(test_train)
```

Una vez tenemos las categoras dummies y las hemos unido y aplicado la correlacin BoxCox aplicamos Boruta.

### Boruta

```{r}

set.seed(197)
bor.df <- test_train[1:1458,]
bor.results <- Boruta(bor.df[,-33], bor.df$SalePrice,
                      maxRuns = 338, doTrace = 0)
```

Vemos los resultados obtenidos tras aplicar Boruta.

```{r}
cat("Las caracter铆sticas seleccionadas tras aplicar Boruta son:")
print(bor.results)
cat("Podemos ver odas las variables obtenidas mostrando el resultado final con $finalDecision.")
#print(bor.results.prove$finalDecision)
```

Una vez sabemos cuales son las caracter铆sticas con las que debemos quedarnos, procedemos a seleccionar esa lista de caracter铆sticas, de las cuales obtendremos tanto las importante como las tentativas.

```{r}
selected.carac <- getSelectedAttributes(bor.results, withTentative = T)
print(selected.carac)
```

Una vez tenemos seleccionadas las caracter铆sticas que son relevantes, en test_train nos quedamos con estas:

```{r}
train_test <- test_train[,selected.carac]
train_test$SalePrice <- t_t
```

# Construcci贸n de modelos y evaluaci贸n

Dividimos en train y test otra vez para poder evaluar los modelos:

```{r}
training <- train_test[1:1458, ]
testing <- train_test[1459:2917, ]
# Partici贸n para el entrenamiendo de los modelos
pTrain <- createDataPartition(y = training$SalePrice, p = 0.8, list = FALSE)
Training <- training[pTrain, ]
Validation <- training[-pTrain, ]
```

## Modelos
```{r}
library(Metrics)
library(gbm)
library(xgboost)
library(glmnet)
library(caret)

```

### Gradient Boosting

```{r}
set.seed(371)
gbmFit <- gbm(SalePrice ~., data = Training, shrinkage = 0.05, interaction.depth=3,
              bag.fraction = 0.5012, n.minobsinnode = 10,
              cv.folds = 5, verbose=FALSE, distribution = "gaussian", n.trees = 3000)

predsp1 <- predict(gbmFit, newdata = Validation)
rmse(Validation$SalePrice, predsp1)
```
 0.1175374
```{r}
set.seed(371)
gbmFited <- gbm(SalePrice ~., data = training, shrinkage = 0.05, interaction.depth=3,
              bag.fraction = 0.5, n.minobsinnode = 10,
              cv.folds = 5, verbose=FALSE, distribution = "gaussian", n.trees = 3000)
predsGBM <- exp(predict(gbmFited, newdata = testing)) - 1
```

Guardamos los resultados de GBM.

```{r}
dfGBM <- data.frame(Id = test_ID, SalePrice = predsGBM)
write.csv(dfGBM, "1stGBM.csv", row.names = FALSE)
```

### Lasso

```{r}
set.seed(793)
lassoFit <- cv.glmnet(x = as.matrix(Training[,-128]), y = as.matrix(Training[,128]), family = "gaussian",
                      alpha =0.1655, nfolds = 5)
predsp2 <- predict(lassoFit, newx = as.matrix(Validation[,-128]), s = "lambda.min")
rmse(Validation$SalePrice, predsp2)
```


Una vez hemos comprobado el funcionamiento de Lasso procedemos a ejecutarlo para los datos de training totales:
```{r}
set.seed(793)
lasso.fit <- cv.glmnet(x = as.matrix(training[,-128]), y = as.matrix(training[,128]), nfolds = 5
                       , family = "gaussian", alpha =0.1655)
preds.lasso <- exp(predict(lasso.fit, newx = as.matrix(testing[,-128]), s = "lambda.min")) - 1 
```

Y almacenamos los resultados:
```{r}
df.lasso <- data.frame(Id = test_ID, SalePrice = as.numeric(preds.lasso))
write.csv(df.lasso, "1stLasso.csv", row.names = FALSE)
```

### Xgboost


```{r}
set.seed(371)
xgbtrain <- xgb.DMatrix(data = as.matrix(training[,-128]), label = as.matrix(training$SalePrice))
params <- list(booster = "gbtree", objetive = "reg:linear", eta = 0.05, gamma = 0.0468,
               max_depth = 3, min_child_weight = 1.782, subsample = 0.5213,
               colsample_bytree=0.4578, eval_metric='rmse', nthread = 4, verbose=TRUE, seed=371,
               alpha = 0.4640, lambda = 0.8574, silent = 1)

xgbcvFited <- xgb.cv( params = params, data = xgbtrain, nrounds = 3200, nfold = 5, stratified = T, print_every_n = 100,
                 early_stopping_rounds = 10, maximize =F, prediction = TRUE)
```
Obtener pred para ensamble.
```{r}
set.seed(371)
dtrain <- xgb.DMatrix(data = as.matrix(Training[-128]), label = as.matrix(Training$SalePrice))
xgbFit <- xgboost(data = dtrain, nfold = 5, labels = as.matrix(Training$SalePrice),
                  nrounds = 2400, verbose = FALSE, objetive = "reg:linear", eval_metric = 'rmse',
                  nthread = 4, eta = 0.0075, gamma = 0.0468, max_depth = 3, min_child_weight = 1.782,
                  subsample = 0.5213, colsample_bytree = 0.4588, silent = 1, alpha = 0.4640, lambda = 0.8574)
pred <- predict(xgbFit, newdata = as.matrix(Validation[,-128]))
rmse(Validation$SalePrice, pred)
```

Preparamos Xgboost para subir.

```{r}
set.seed(371)
xgbtrain <- xgb.DMatrix(data = as.matrix(training[,-128]), label = as.matrix(training$SalePrice))

xgbFit <- xgboost(data = xgbtrain, nfold = 5, labels = as.matrix(training$SalePrice),
                  nrounds = 2400, verbose = FALSE, objetive = "reg:linear", eval_metric = 'rmse',
                  nthread = 4, eta = 0.0075, gamma = 0.0468, max_depth = 3, min_child_weight = 1.782,
                  subsample = 0.5213, colsample_bytree = 0.4588, silent = 1, alpha = 0.4640, lambda = 0.8574)
predXGB <- exp(predict(xgbFit, newdata = as.matrix(testing[, -128]))) - 1 
```

Guardamos los resultados
```{r}
df <- data.frame(Id = test_ID, SalePrice = predXGB)
write.csv(df, "NewXgboost.csv", row.names = FALSE)
```


Ensamble de los modelos utilizados.
```{r}
rmse(Validation$SalePrice, (pred + predsp1 + predsp2)/3)
```

Ensamble de los algoritmos utilizados.
```{r}
preds.ensamble <- 0.4 * predXGB + 0.4 * predsGBM + 0.2 * as.numeric(preds.lasso)
ensamble.df<- data.frame(Id = test_ID, SalePrice = preds.ensamble)
write.csv(ensamble.df, "WORstEnsambleGbmRfXgb.csv", row.names = FALSE)
```